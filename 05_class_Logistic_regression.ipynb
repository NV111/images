{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l0_check(a: np.array):\n",
    "    b = np.array(a)\n",
    "    for i in range(len(b)):\n",
    "        if (b[i] != 0):\n",
    "            b[i] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_FOR_MODEL = \"05_class_Logistic_regression_model.txt\"\n",
    "class LogisticRegression:\n",
    "    def __init__(self, alpha: float, l0: float, l1: float, l2: float, \n",
    "                 stop_iter: float, stop_delta: float, verbose: bool, model: object):\n",
    "        '''\n",
    "        Linerar Regression object constructor\n",
    "        \n",
    "        :param alpha: alpha param for gradient descent\n",
    "        :param l0: L0 regularization coefficient\n",
    "        :param l1: L1 regularization coefficient\n",
    "        :param l2: L2 regularization coefficient\n",
    "        :param stop_iter: maximum iterations of traing\n",
    "        :param stop_delta: stop iteration delta\n",
    "        :param verbose: show verbose information\n",
    "        :param model: get params from other model\n",
    "        '''\n",
    "        if (model == None):\n",
    "            self.alpha = alpha\n",
    "            self.l0 = l0\n",
    "            self.l1 = l1\n",
    "            self.l2 = l2\n",
    "            self.stop_iter = stop_iter\n",
    "            self.stop_delta = stop_delta\n",
    "            self.verbose = verbose\n",
    "            self.model = model\n",
    "        else:            \n",
    "            self.alpha = model['alpha']\n",
    "            self.l0 = model['l0']\n",
    "            self.l1 = model['l1']\n",
    "            self.l2 = model['l2']\n",
    "            self.stop_iter = model['stop_iter']\n",
    "            self.stop_delta = model['stop_delta']\n",
    "            self.verbose = model['verbose']\n",
    "            self.model = model['model']\n",
    "    \n",
    "    def train(self, X: np.array, y: np.array, warm_start: bool):\n",
    "        '''\n",
    "        Fit Linear Regression params\n",
    "        \n",
    "        :param X: training data\n",
    "        :param y: training ansewers\n",
    "        :param warm_start: must be set True to continue training, false to reset params\n",
    "        '''\n",
    "        \n",
    "        if (warm_start == False):\n",
    "            self.w = np.random.rand(len(X[0]))\n",
    "        else:\n",
    "            file = open(FILE_FOR_MODEL, 'rb')\n",
    "            self.w = cPickle.load(file)\n",
    "            file.close()\n",
    "            \n",
    "        indicator_for_cycle_exit_criterion = 1   \n",
    "        \n",
    "        for j in range(int(self.stop_iter)):\n",
    "            \n",
    "            grad = X.T.dot(y-1/(1+np.exp(X.dot(self.w.T))))/len(y) + self.l0*l0_check(self.w) + self.l1*np.sign(self.w) + self.l2*2*self.w\n",
    "            self.w = self.w - self.alpha*grad\n",
    "            j += 1\n",
    "            \n",
    "            if (self.verbose == True):\n",
    "                if (j % 100000 == 0):\n",
    "                    print('iteration = ', j, 'vect =', self.w, 'grad =', grad)\n",
    "            \n",
    "            if(np.linalg.norm(grad) < self.stop_delta):\n",
    "                indicator_for_cycle_exit_criterion = 0   \n",
    "                print('|grad| < stop_delta')\n",
    "                break\n",
    "                \n",
    "        if (indicator_for_cycle_exit_criterion):\n",
    "            print(self.stop_iter, 'iterations done')\n",
    "        \n",
    "        file = open(FILE_FOR_MODEL, 'wb')\n",
    "        cPickle.dump(self.w, file)\n",
    "        file.close()\n",
    "        print('train results saved in', FILE_FOR_MODEL)\n",
    "            \n",
    "    \n",
    "    def get_params(self) -> dict:\n",
    "        '''\n",
    "        Return model params\n",
    "        \n",
    "        :return: dict of model params\n",
    "        '''\n",
    "        dict_params = {}\n",
    "        dict_params['alpha'] = self.alpha\n",
    "        dict_params['l0'] = self.l0\n",
    "        dict_params['l1'] = self.l1\n",
    "        dict_params['l2'] = self.l2\n",
    "        dict_params['stop_iter'] = self.stop_iter\n",
    "        dict_params['stop_delta'] = self.stop_delta\n",
    "        dict_params['verbose'] = self.verbose\n",
    "        dict_params['model'] = self.model\n",
    "        return dict_params\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        file = open(FILE_FOR_MODEL, 'rb')\n",
    "        self.w = cPickle.load(file)\n",
    "        file.close()\n",
    "        '''\n",
    "        Predit answers on given data\n",
    "        \n",
    "        :param X: data\n",
    "        :return: predicted answers\n",
    "        '''\n",
    "        return 1/(1+np.exp(X.dot(self.w.T)))\n",
    "    \n",
    "    def test(self, X: np.array, y: np.array, metric=None) -> float:\n",
    "        file = open(FILE_FOR_MODEL, 'rb')\n",
    "        self.w = cPickle.load(file)\n",
    "        file.close()\n",
    "        '''\n",
    "        Test the model\n",
    "        \n",
    "        :param X: test data\n",
    "        :param y: test answers\n",
    "        :param metric: must be a function of 2 numpy arrays. If None, MSE is used.\n",
    "        :return: metric value\n",
    "        '''\n",
    "        return (1/(1+np.exp(X.dot(self.w.T))) - y).dot((1/(1+np.exp(X.dot(self.w.T))) - y).T)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logr = LogisticRegression(alpha=1e-4, l0=0., l1=1., l2=0., stop_iter=1e6, stop_delta=1e-4, verbose=False, model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_log = np.array([[1,-100],\n",
    "                    [1,-80],\n",
    "                    [1,-50],\n",
    "                    [1,-60],\n",
    "                    [1,-40],\n",
    "                    [1,20],\n",
    "                    [1,60],\n",
    "                    [1,90],\n",
    "                    [1,70],\n",
    "                    [1,80],\n",
    "                    [1,-70],\n",
    "                    [1,100]])\n",
    "y_log = np.array([0,0,0,0, 0,0,1,1,1,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =  100000 vect = [ 1.20862322 -0.06592539] grad = [-0.03727316  0.00052396]\n",
      "iteration =  200000 vect = [ 1.547615   -0.07111928] grad = [-0.03099486  0.00050984]\n",
      "iteration =  300000 vect = [ 1.83531546 -0.07607385] grad = [-0.02679134  0.00048016]\n",
      "iteration =  400000 vect = [ 2.0871894  -0.08071733] grad = [-0.02372705  0.00044867]\n",
      "iteration =  500000 vect = [ 2.31214622 -0.08505351] grad = [-0.02135627  0.00041897]\n",
      "iteration =  600000 vect = [ 2.5158367  -0.08910555] grad = [-0.01944522  0.00039189]\n",
      "iteration =  700000 vect = [ 2.70213027 -0.09290013] grad = [-0.01785973  0.00036746]\n",
      "iteration =  800000 vect = [ 2.87383598 -0.09646282] grad = [-0.01651659  0.00034547]\n",
      "iteration =  900000 vect = [ 3.0330845  -0.09981672] grad = [-0.01536075  0.00032566]\n",
      "iteration =  1000000 vect = [ 3.18154624 -0.10298234] grad = [-0.01435385  0.00030777]\n",
      "1000000.0 iterations done\n",
      "train results saved in 05_class_Logistic_regression_model.txt\n"
     ]
    }
   ],
   "source": [
    "logr.verbose = True\n",
    "logr.train(X=x_log, y=y_log, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =  100000 vect = [  9.63816342e-05  -4.80415475e-02] grad = [ -1.07674242e+00   4.42298340e-06]\n",
      "iteration =  200000 vect = [ -8.92480281e-05  -4.80415470e-02] grad = [  9.23258330e-01   7.24484908e-07]\n",
      "iteration =  300000 vect = [ -7.49064816e-05  -4.80415510e-02] grad = [  9.23259076e-01  -3.58480715e-06]\n",
      "iteration =  400000 vect = [ -6.05901599e-05  -4.80415542e-02] grad = [  9.23259820e-01  -7.77565912e-06]\n",
      "iteration =  500000 vect = [ -4.62377750e-05  -4.80415565e-02] grad = [  9.23260565e-01  -1.18512082e-05]\n",
      "iteration =  600000 vect = [ -3.18679214e-05  -4.80415434e-02] grad = [  9.23261308e-01  -1.37741597e-05]\n",
      "iteration =  700000 vect = [ -1.75418416e-05  -4.80415366e-02] grad = [  9.23262050e-01  -1.65705523e-05]\n",
      "iteration =  800000 vect = [ -3.22339854e-06  -4.80415345e-02] grad = [  9.23262793e-01  -2.00238278e-05]\n",
      "iteration =  900000 vect = [  1.11288108e-05  -4.80415331e-02] grad = [  9.23263537e-01  -2.35824302e-05]\n",
      "iteration =  1000000 vect = [  2.55022943e-05  -4.80415368e-02] grad = [ -1.07674610e+00   2.44681438e-05]\n",
      "1000000.0 iterations done\n",
      "train results saved in 05_class_Logistic_regression_model.txt\n"
     ]
    }
   ],
   "source": [
    "logr.verbose = True\n",
    "logr.train(X=x_log, y=y_log, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'l0': 0.0,\n",
       " 'l1': 0.0,\n",
       " 'l2': 0.0,\n",
       " 'model': None,\n",
       " 'stop_delta': 0.01,\n",
       " 'stop_iter': 1000000.0,\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logr2=LogisticRegression(**logr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x2_log = np.array([[1,-121],\n",
    "                    [1,-81],\n",
    "                    [1,-58],\n",
    "                    [1,-69],\n",
    "                    [1,-35],\n",
    "                    [1,25],\n",
    "                    [1,76],\n",
    "                    [1,87],\n",
    "                    [1,66],\n",
    "                    [1,60],\n",
    "                    [1,-70],\n",
    "                    [1,100]])\n",
    "y2_log = np.array([0,0,0,0, 0,0,1,1,1,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "iteration =  100000 vect = [ 1.21215986 -0.06597516] grad = [-0.03719863  0.000524  ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-71a40e521f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogr2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-768cde7f288a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, warm_start)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"05_class_Logistic_regression_model.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logr2.train(X=x_log, y=y_log, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "400000\n",
      "iteration =  400000 vect = [ 2.08944178 -0.08075993] grad = [-0.02370176  0.00044838]\n",
      "500000\n",
      "iteration =  500000 vect = [ 2.31417367 -0.08509329] grad = [-0.02133623  0.0004187 ]\n",
      "600000\n",
      "iteration =  600000 vect = [ 2.51768282 -0.08914276] grad = [-0.01942879  0.00039164]\n",
      "700000\n",
      "iteration =  700000 vect = [ 2.70382593 -0.09293502] grad = [-0.01784594  0.00036724]\n",
      "800000\n",
      "iteration =  800000 vect = [ 2.87540415 -0.09649562] grad = [-0.0165048   0.00034527]\n",
      "900000\n",
      "iteration =  900000 vect = [ 3.03454297 -0.09984765] grad = [-0.01535054  0.00032548]\n",
      "1000000\n",
      "iteration =  1000000 vect = [ 3.18290913 -0.10301157] grad = [-0.0143449   0.00030761]\n",
      "1000000.0 iterations done\n"
     ]
    }
   ],
   "source": [
    "logr2.train(X=x_log, y=y_log, warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010614571805499589"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr2.test(X=x2_log, y=y2_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
